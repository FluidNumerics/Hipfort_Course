{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283778d2-47bd-4d22-a576-8fa737e200a2",
   "metadata": {},
   "source": [
    "# Fundamentals of accelerated computing\n",
    "\n",
    "In order to understand how to use GPU's effectively with Fortran, we need to get a fundamental understanding of how accelerated computing works. The following sections introduce the fundamental concepts of working with HIP and accelerators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897245e2-07dc-429e-80ca-3602a264d2e1",
   "metadata": {},
   "source": [
    "## A brief history of GPU's for scientific computing.\n",
    "\n",
    "Graphics Processing Units (**GPU**'s) originated with the need to quickly perform math operations for rendering a 3D scene for display on a screen, such as in a game. Rendering pixels is an readily parallelizable operation, and the compute operation can be performed in parallel over the available hardware units. Originally these units used specialized silicon to perform the rendering calculations in parallel, however as the complexity of algorithms increased the hardware units became more generalised and programmable. Demand for the best frame rates in games drove performance, and this resulted in vendors providing GPU's with ever higher compute performance and memory bandwidth.\n",
    "\n",
    "In 2004 the graphics card company ATI launched \"Close To Metal\", the first commercial solution for performing scientific calculations in parallel over General Purpose Graphics Processing Units (GPGPU's). This was followed by NVIDIA's CUDA in 2007 and Apple/Khrono's OpenCL in 2009, Apple's Metal in 2014 and AMD's HIP in 2016. Frameworks such as these enabled scientific calculations to be performed on the GPU at a rate that is often much faster than on CPU's. GPU's were packaged as discrete devices, separate from the CPU and connected to the host over a connection such as PCI Express.\n",
    "\n",
    "In recent times, accelerating the training and inference operations in artificial intelligence is now the primary economic driver for compute performance in GPU's. Recent designs such as AMD's Mi300 and NVIDIA's Grace Hopper integrate both CPU's and GPU's in the same processor die along with high bandwidth memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d9d42-d44e-41c8-9f7c-b42fa7a2fa33",
   "metadata": {},
   "source": [
    "## Introduction to HIP\n",
    "\n",
    "HIP stands for the Heterogeneous Interface for Portability. HIP is part of ROCM, AMD's competitor to CUDA, and aims to make GPU's accessible through providing  a subset of capability formed from the fusion of the driver and runtime API's in CUDA. HIP calls have their own prefix (i.e **hipMalloc** instead of **cudaMalloc**) and they can serve as a **very thin** layer over a vendor's GPU library calls when using their backend. As such, this design philosophy currently allows HIP programs to use either an AMD, NVIDIA, or even an Intel accelerator as the compute device, while allowing the use of vendor-specific debugging and performance tools. HIP has a number of benefits that include:\n",
    "\n",
    "* A single source for programs and kernels.\n",
    "* The ability to use an Intel, NVIDIA, or AMD compute devices at full performance.\n",
    "* Easy-to-use API that is familiar in many ways to CUDA, with the ability to benefit from knowledge in established literature on GPU computing with CUDA.\n",
    "* Tools available to port code from CUDA to HIP.\n",
    "\n",
    "There are also some challenges that must be considered when considering using HIP for your project.\n",
    "\n",
    "* The number of officially-supported devices and operating systems is quite low. See this [page](https://rocm.docs.amd.com/en/latest/release/gpu_os_support.html) for devices that are officially supported. Other recent AMD devices do generally work with ROCm, but with varying levels of support. \n",
    "* Only one type of compute device (limited to a single vendor) is accessible to a HIP program at runtime. In order to change vendors or compute devices the program must be recompiled with a different backend.\n",
    "* The HIP API is still on the way to maturity. As such it is anticipated there will be bugs and things that don't work properly.\n",
    "* Not every CUDA API call is supported in HIP.\n",
    "\n",
    "## Introduction to hipfort\n",
    "\n",
    "Hipfort is the Fortran interface to HIP, and provides a way to access the compute power of a GPU and HIP and ROCm compute libraries from a Fortran program. It is designed to provide access to HIP and ROCm library calls at the level of around ROCm 4.5. Supported libraries include:\n",
    "\n",
    "* HIP\n",
    "* hipBLAS and rocBLAS (Basic Linear Algebra)\n",
    "* hipFFT and rocFFT (Fast Fourier Transforms)\n",
    "* hipRAND and rocRAND (Random number generation)\n",
    "* hipSOLVER and rocSOLVER (Linear algebra solvers, AMD backends only at this point)\n",
    "* hipSPARSE and rocSPARSE (Implementation and tools to work with spare matrices.)\n",
    "\n",
    "The hip**\\*** libraries can use multiple backends, while the roc**\\*** libraries are specific to AMD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553267f3-249b-49a6-91d4-996f3933dd52",
   "metadata": {},
   "source": [
    "## GPU computing hardware\n",
    "\n",
    "### Compute\n",
    "### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8395281c-d9b3-4bf6-a428-8675cc077d8b",
   "metadata": {},
   "source": [
    "## Anatomy of an accelerated application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2442825-43f1-4d18-8caa-f2eac518e773",
   "metadata": {},
   "source": [
    "<address>\n",
    "Written by Dr. Toby Potter of <a href=\"https://www.pelagos-consulting.com\">Pelagos Consulting and Education</a> and Dr. Joe Schoonover from <a href=\"https://www.fluidnumerics.com\">Fluid Numerics</a>. All trademarks mentioned in this page are the property of their prospective owners.\n",
    "</address> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3157b2-53b0-41fa-8cdf-422aecea2efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
