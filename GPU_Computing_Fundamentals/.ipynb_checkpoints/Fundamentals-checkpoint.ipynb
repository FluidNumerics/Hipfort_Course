{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283778d2-47bd-4d22-a576-8fa737e200a2",
   "metadata": {},
   "source": [
    "# Fundamentals of accelerated computing\n",
    "\n",
    "In order to understand how to use GPU's effectively with Fortran, we need to get a fundamental understanding of how accelerated computing works. The following sections introduce the fundamental concepts of working with HIP and accelerators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897245e2-07dc-429e-80ca-3602a264d2e1",
   "metadata": {},
   "source": [
    "## A brief history of GPU's for scientific computing.\n",
    "\n",
    "Graphics Processing Units (**GPU**'s) originated with the need to quickly perform math operations for rendering a 3D scene for display on a screen, such as in a game. Rendering pixels is an readily parallelizable operation, and the compute operation can be performed in parallel over the available hardware units. Originally these units used specialized silicon to perform the rendering calculations in parallel, however as the complexity of algorithms increased the hardware units became more generalised and programmable. Demand for the best frame rates in games drove performance, and this resulted in vendors providing GPU's with ever higher compute performance and memory bandwidth.\n",
    "\n",
    "In 2004 the graphics card company ATI launched \"Close To Metal\", the first commercial solution for performing scientific calculations in parallel over General Purpose Graphics Processing Units (GPGPU's). This was followed by NVIDIA's CUDA in 2007 and Apple/Khrono's OpenCL in 2009, Apple's Metal in 2014 and AMD's HIP in 2016. Frameworks such as these enabled scientific calculations to be performed on the GPU at a rate that is often much faster than on CPU's. GPU's were packaged as discrete devices, separate from the CPU and connected to the host over a connection such as PCI Express.\n",
    "\n",
    "In recent times, accelerating the training and inference operations in artificial intelligence is now the primary economic driver for compute performance in GPU's. Recent designs such as AMD's Mi300 and NVIDIA's Grace Hopper integrate both CPU's and GPU's in the same processor die along with high bandwidth memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d9d42-d44e-41c8-9f7c-b42fa7a2fa33",
   "metadata": {},
   "source": [
    "## Introduction to HIP\n",
    "\n",
    "HIP stands for the Heterogeneous Interface for Portability. HIP is AMD's competitor to CUDA, and aims to make GPU's accessible through providing  a subset of capability formed from the fusion of both driver and runtime API's in CUDA. HIP calls have their own prefix (i.e **hipMalloc** instead of **cudaMalloc**) and these allow HIP programs to use either an AMD, NVIDIA, or even and Intel accelerator as the compute device.\n",
    "\n",
    "\n",
    "* Provide feature parity with CUDA\n",
    "* Very thin layer over CUDA\n",
    "* ROCm as the AMD specific stack\n",
    "* Subset of CUDA API\n",
    "* Single source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553267f3-249b-49a6-91d4-996f3933dd52",
   "metadata": {},
   "source": [
    "## GPU computing hardware\n",
    "\n",
    "### Compute\n",
    "### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8395281c-d9b3-4bf6-a428-8675cc077d8b",
   "metadata": {},
   "source": [
    "## Anatomy of an accelerated application"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
