{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283778d2-47bd-4d22-a576-8fa737e200a2",
   "metadata": {},
   "source": [
    "# Fundamentals of accelerated computing\n",
    "\n",
    "In order to understand how to use GPU's effectively with Fortran, we need to get a fundamental understanding of how accelerated computing works. The following sections introduce the fundamental concepts of working with HIP and accelerators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897245e2-07dc-429e-80ca-3602a264d2e1",
   "metadata": {},
   "source": [
    "## A brief history of GPU's for scientific computing.\n",
    "\n",
    "Graphics Processing Units (**GPU**'s) originated with the need to quickly perform math operations for rendering a 3D scene for display on a screen, such as in a game. Rendering pixels is an readily parallelizable operation, and the compute operation can be performed in parallel over the available hardware units. Originally these units used specialized silicon to perform the rendering calculations in parallel, however as the complexity of algorithms increased the hardware units became more generalised and programmable. Demand for the best frame rates in games drove performance, and this resulted in vendors providing GPU's with ever higher compute performance and memory bandwidth.\n",
    "\n",
    "In 2004 the graphics card company ATI launched \"Close To Metal\", the first commercial solution for performing scientific calculations in parallel over General Purpose Graphics Processing Units (GPGPU's). This was followed by NVIDIA's CUDA in 2007 and Apple/Khrono's OpenCL in 2009, Apple's Metal in 2014 and AMD's HIP in 2016. Frameworks such as these enabled scientific calculations to be performed on the GPU at a rate that is often much faster than on CPU's. GPU's were packaged as discrete devices, separate from the CPU and connected to the host over a connection such as PCI Express.\n",
    "\n",
    "In recent times, accelerating the training and inference operations in artificial intelligence is now the primary economic driver for compute performance in GPU's. Recent designs such as AMD's Mi300 and NVIDIA's Grace Hopper integrate both CPU's and GPU's in the same processor die along with high bandwidth memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56c806-ec80-4415-bce1-40369f34d2d7",
   "metadata": {},
   "source": [
    "## An overview of GPU hardware\n",
    "\n",
    "The purpose of a GPU is to bring data in, execute instructions on that data, and pass that data back out again. In a GPU compute die there are many of the same elements that are also present in a CPU.  If we define a **core** as \"that which schedules instructions\", then a **Compute** Unit in a GPU can be likened to a core in a CPU. The term Compute Unit is from OpenCL and AMD terminology, they are also known as **Stream multiprocessors** (NVIDIA), **XE Cores** (Intel). Compute units schedule instructions to be executed in **lock-step** over teams of hardware threads that are made available by the compute unit. A GPU has **many** compute units, and within each compute unit resides a number of SIMD-like math units (usually 64-128), and  each unit executes instructions in a hardware thread. These units execute instructions over vectors of integers and floats, and are also known as **CUDA cores** in NVIDIA terminology, **Shader cores** for AMD, and **XE** vector engines for Intel. A GPU can have many thousands of hardware threads available for executing instructions in parallel at the same time.\n",
    "\n",
    "GPU's use a SIMT execution model where an instruction issued by the compute unit is performed in parallel and in lock-step, over every hardware thread in the team, by the available math units. Thread teams are usually 32-64 **lanes** wide, and go by the name **wavefronts** (AMD) or **warps** (NVIDIA). A compute unit can have many teams **active** (ready to execute instructions), and can switch between those thread teams with minimal overhead if a thread team stalls while waiting for memory. **Occpuancy** is the ratio between how many threads **are** active vs how many **can be** active. \n",
    "\n",
    "**Tensor units** may also be present in a compute unit. They are specialised silicon (also known as Tensor Cores or Matrix Engines) that execute bulk linear algebra operations over arrays of numbers. Tensor units are often accessed only through special instructions in the vendor's kernel syntax.\n",
    "\n",
    "<figure style=\"margin: 1em; margin-left:auto; margin-right:auto; width:70%;\">\n",
    "    <img src=\"../images/GPU_components.svg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Simplified diagram of the components in a GPU die.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Each hardware thread in a Compute Unit has access to a portion of **register memory**. Teams of hardware threads in a compute unit jointly have access to **Shared memory**, and all hardware threads have access to **Global** memory, which is the largest and slowest memory space on the compute device. The L1 cache is usually specific to the compute unit, while the L2 cache is available to all compute units. It is uncommon to see an L3 cache being utilised in a GPU.\n",
    "\n",
    "### Data movement between caches\n",
    "\n",
    "Memory that is accessed hardware threads is shuffled between Global memory and the caches in discrete chunks called **cache lines**, usually 64-128 Bytes in size. It is **vital** for performance in high performance computing to try and reuse memory in a cache line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6bef9-ceed-4247-abfe-2bb2b3da4aca",
   "metadata": {},
   "source": [
    "## Software concepts \n",
    "\n",
    "### Kernel\n",
    "\n",
    "A software thread is a sequence of instructions that are executed by a hardware thread. There can be many software threads, and the operating system schedules software threads to run on hardware threads and thread teams **as they become available**. In GPU computing we perform compute operations using **kernels**, small lightweight software threads that are designed to run in parallel over the available hardware threads in a GPU.\n",
    "\n",
    "Below is an example HIP/CUDA kernel that performs the math operation **C**\\[i0\\]=**A**\\[i0\\]+**B**\\[i0\\] at a single location **i0** of the vectors.\n",
    "\n",
    "```C++\n",
    "__global__ void vector_add(\n",
    "    float* A,\n",
    "    float* B,\n",
    "    float* C,\n",
    "    int N) {\n",
    "\n",
    "    // Get the position\n",
    "    size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (i0<N) {\n",
    "        C[i0] = A[i0] + B[i0];\n",
    "    }\n",
    "    \n",
    "}\n",
    "```\n",
    "\n",
    "This kernel is designed to perform the add operation on a **single element** of the vectors **C**, **B**, and **A** at index **i0**. We run this kernel **in parallel** over the hardware threads in a GPU as they become available. Each kernel is responsible for just one element of the vector addition.\n",
    "\n",
    "### The Grid\n",
    "\n",
    "When we launch a kernel we need to specify how many kernel instances are needed to cover the domain of the vector. The **Grid** is a rectangular and discretized execution space of up to three dimensions, whose size we specify at runtime. The vendor's software runtime makes sure a kernel is executed **at every point** in this space. Grids are divided into **Blocks**, and every Block is divided into **Threads**. There usually must be an integer number of blocks covering the domain, and this means sometimes we need to specify a Grid that is larger than what we need. In terms of the Grid, a Thread is an instance of a kernel running in a software thread on the machinery provided by a hardware thread. Below is an example Grid. It is of size (x,y,z) = (16,16,2). In the Grid are 8 blocks, each of size (8,8,1). Therefore there are (2,2,2) blocks along each dimension of the grid. \n",
    "\n",
    "At runtime, kernel instances are mapped by the compute framework to available hardware threads. In terms of OpenCL, HIP, and CUDA, the Grid is **column-major** in the sense that Threads are neighbours along the first dimension of the grid. Since Threads are mapped to hardware threads who work in thread team, the most optimal choices for the block size is naturally a **multiple** of the number of hardware threads in a hardware thread team on the GPU. Notice in the diagram that some blocks are not finished. This is to show that we must not make any assumptions on the order in which blocks are executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd9f6a-c6eb-47d3-a28d-5ee8e4093d4d",
   "metadata": {},
   "source": [
    "<figure style=\"margin: 1em; margin-left:auto; margin-right:auto; width:90%;\">\n",
    "    <img src=\"../images/Grid.svg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">A Grid in the context of GPU computing. Grids are made up of Blocks and Blocks are made up of Threads</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb07acd5-dbef-4223-b2e3-eb0b95106b71",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Memory allocations and memory access from a kernel\n",
    "\n",
    "A GPU computing framework provides the means to **allocate** and **de-allocate** memory on the compute device. A (contiguous) memory allocation is thought of as being one really long array of Bytes whose memory is interpreted as different data types. In the diagram below is a memory allocation of 16 Bytes. We can interpret this memory in a **variety of ways**, here we interpret it as 16 int8, 8 x fp16, 4 x fp32, or 2 x fp64.\n",
    "\n",
    "<figure style=\"margin: 1em; margin-left:auto; margin-right:auto; width:70%;\">\n",
    "    <img src=\"../images/Memory.svg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">The same memory allocation interpreted in different ways.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The compute runtime allows us to choose if this memory is in a **private** memory space on the GPU, which  means it is only accessible to a Thread; or we can allocate it in a **shared** memory space accessible to every Thread in a Block. Finally, we can allocate the memory in a **global** memory space which is accesssible from every Thread.\n",
    "\n",
    "Through a set of query instructions, every kernel has the means to find out:\n",
    "\n",
    "* The number of Blocks in each dimension of the grid.\n",
    "* The number of Threads along each dimension of the Block.\n",
    "* The coordinates of the kernel (Thread) within the Block.\n",
    "* The coordinates of the Block within the Grid.\n",
    "\n",
    "A kernel can use these coordinates to calculate an offset into any portion of allocated memory.\n",
    "\n",
    "<figure style=\"margin: 1em; margin-left:auto; margin-right:auto; width:70%;\">\n",
    "    <img src=\"../images/Memory_offset.svg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Using a kernel's position within a Grid to calculate a memory offset within a memory allocation.</figcaption>\n",
    "</figure>\n",
    "\n",
    "It is the programmer's responsibility to make sure that the kernel does not access memory that is beyond the memory allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80612b57-8abd-4dbc-b51c-8fe3cfcb5e14",
   "metadata": {},
   "source": [
    "### Multidimensional arrays and memory allocations\n",
    "\n",
    "In scientific computing we often work with multi-dimensional arrays. When writing GPU kernels it is helpful to know how to map a coordinate vector **C** in a multidimensional array to a position in a memory allocation. In the diagram below is an array allocation of 16 elements and below  it are multi-dimensional arrays of size (2,4,2) whose elements are stored in the allocation. \n",
    "\n",
    "<figure style=\"margin: 1em; margin-left:auto; margin-right:auto; width:70%;\">\n",
    "    <img src=\"../images/Multidimensional_arrays.svg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Using a kernel's position within a Grid to calculate a memory offset within a memory allocation.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2c1148-75db-49f6-90c6-9ee76d64fcac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "If you look at each of the multi-dimensional arrays above, you can see that a **step of one element** in each dimension corresponds to a **stride** of a fixed number of elements in the array allocation. There is a stride for each dimension of the array, and we call **S** the **stride vector** for the array. There are two main methods for ordering the elements in multi-dimensional arrays. Fortran uses **column-major** ordering where dimension 0 has a stride of 1. Languages like C and Python generally use **row-major** ordering, where the last dimension has a stride of 1. \n",
    "\n",
    "The indices of Fortran arrays usually start at 1 by default, while the C/C++ kernel languages used in OpenCL, CUDA, or HIP use arrays whose indices start at 0. If you know the (zero-based) coordinate **C** in the multidimensional vector, then the position $p$ in the array allocation is given by the equation\n",
    "\n",
    "$$p=\\textbf{C} \\cdot \\textbf{S}.$$\n",
    "\n",
    "In the example above lets say we have the coordinates **C=(0,2,1)**. In row-major ordering, the strides vector is **S=(8,2,1)**, therefore the position $p$ in the allocation is\n",
    "\n",
    "$$p=0 \\times 8 + 2 \\times 2 + 1 \\times 1 = 5.$$\n",
    "\n",
    "In column-major ordering the strides vector is **S=(1,2,8)**, therefore for the same coordinates the position $p$ in the allocation is\n",
    "\n",
    "$$p=0 \\times 1 + 2 \\times 2 + 1 \\times 8 = 12.$$\n",
    "\n",
    "#### Constructing a strides vector\n",
    "\n",
    "Constructing a strides vector is straightforward, as shown in the figure below. If you have a vector **N**, the size of the multidimensional array in each dimension, then the strides vector **S** is built by starting with the stride of 1 and multiplying the stride $\\textbf{S}_i$ at index $i$ by the corresponding element in $\\textbf{N}_i$ to get the next element in **S**. For column-major arrays this means building the strides vector forwards, and for row-major arrays this means starting at the last element of the strides vector and working backwards. \n",
    "\n",
    "<figure style=\"margin: 1em; margin-left:auto; margin-right:auto; width:70%;\">\n",
    "    <img src=\"../images/Strides_vector.svg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Constructing a strides vector in both row-major and column-major ordering.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Once you have the strides vector then it is very simple to use it, whenever you need need to step by 1 in any dimension of a multidimensional array then all you need to do is add or substract the corresponding **stride** to get a new position **p** in the allocation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8395281c-d9b3-4bf6-a428-8675cc077d8b",
   "metadata": {},
   "source": [
    "## Anatomy of an accelerated application\n",
    "\n",
    "In a heterogeneous compute system are memory spaces on the compute device and memory spaces on the host. The compute device can usually run problems **much faster** than the host can, so we build an application that uses a compute framework such as OpenCL, CUDA or HIP, to run problems on the compute device and then copy results back to the host.\n",
    "\n",
    "A standard pattern for an accelerated application will have the following elements: \n",
    "\n",
    "\n",
    "1. At program launch compute devices are discovered and initialized.\n",
    "2. Memory spaces are allocated on the compute device\n",
    "3. Kernels are prepared.\n",
    "4. Memory is copied from the host to the compute device\n",
    "5. Kernels are run to perform whatever compute operation is required.\n",
    "6. The output from kernel runs is copied back from the compute device to the host, where it may be checkpointed to disk or sent across the network to other hosts using a message passing library.\n",
    "  \n",
    "**Steps 4-6** are repeated as many times as neccessary until the program is done, then\n",
    "\n",
    "7. Deallocate memory, \n",
    "8. Release resources and exit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2442825-43f1-4d18-8caa-f2eac518e773",
   "metadata": {},
   "source": [
    "<address>\n",
    "Written by Dr. Toby Potter of <a href=\"https://www.pelagos-consulting.com\">Pelagos Consulting and Education</a> and Dr. Joe Schoonover from <a href=\"https://www.fluidnumerics.com\">Fluid Numerics</a>. All trademarks mentioned in this page are the property of their prospective owners.\n",
    "</address> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
