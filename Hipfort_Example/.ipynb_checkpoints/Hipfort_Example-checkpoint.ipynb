{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e72da1e7-84c0-4e8a-81af-6fc0b2351727",
   "metadata": {},
   "source": [
    "# A complete Hipfort application\n",
    "\n",
    "In the **Fortran Refresher** section we covered the essentials of the Fortran language and how to use `subroutines`, `functions`, `pointers`, `modules` as well as how to call C code from Fortran. If this is unfamiliar, then it might be useful to review the material in that section first.\n",
    "\n",
    "From **GPU Computing Fundamentals** section, every accelerated application has the same basic design:\n",
    "\n",
    "1. At program launch compute devices are discovered and initialized.\n",
    "2. Memory spaces are allocated on the compute device.\n",
    "3. Kernels are prepared.\n",
    "4. Memory is copied from the host to the compute device.\n",
    "5. Kernels are run to perform whatever compute operation is required.\n",
    "6. The output from kernel runs is copied back from the compute device to the host. IO may then occur before the next iteration.\n",
    "  \n",
    "**Steps 4-6** are repeated as many times as neccessary until the program is done, then at completion of the program\n",
    "\n",
    "7. Deallocate memory, \n",
    "8. Release resources and exit.\n",
    "\n",
    "## Tensor addition math\n",
    "\n",
    "In this section we are going to walk through each of these steps as part of a complete example with Hipfort, using 2D tensor addition as the basic algorithm. For 2D tensors **A**, **B**, and **C**, each of size (M,N), the following relationship holds true at each index (i,j) in the tensors.\n",
    "\n",
    "$$\n",
    "A(i,j)+B(i,j)=C(i,j)\n",
    "$$\n",
    "\n",
    "In the prior **Fortran Refresher** section we used CPU code in Fortran and C to compute the answer $C(i)$ for 1D tensor addition. In this example we are going to use a HIP Kernel on the GPU to compute the answer $C(i,j)$ at every location in **C**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff9d013-3e39-4f66-a1be-2e2409f460d1",
   "metadata": {},
   "source": [
    "## Example applications\n",
    "\n",
    "In HIP we need a way to get a handle on the memory allocations that are on the compute device. Hipfort can use either a C pointer (`type(c_ptr)`) or a Fortran `pointer` as a handle to the memory allocations on the GPU. The methods of working with each type are subtly different though. In the applications \n",
    "\n",
    "* [tensoradd_hip_cptr.f90](tensoradd_hip_cptr.f90)\n",
    "* [tensoradd_hip_fptr.f90](tensoradd_hip_fptr.f90)\n",
    "\n",
    "we use C pointers and Fortran pointers to perform 2D tensor addition. It will be helpful to have **both files open** at the same time for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dd78fc-3fc9-43e3-8fda-ba5cd6f5d2cf",
   "metadata": {},
   "source": [
    "## Use the Hipfort API\n",
    "\n",
    "Access to all Hipfort functions is via the `hipfort` and `hipfort_check` modules. We bring those modules in along with others at the beginning of the program.\n",
    "\n",
    "```Fortran\n",
    "    ! HIP modules\n",
    "    use hipfort\n",
    "    use hipfort_check\n",
    "```\n",
    "\n",
    "## Check HIP API calls\n",
    "\n",
    "Hipfort functions usually have a **return type** that we can check to make sure everything worked ok. If these checks are **not performed** some functions will continue even though there has been a **silent failure**.  It is therefore **best practice** to **always** the check the return type from HIP calls. The `hipfort_check` module defines a subroutine called `hipcheck` that we can use to wrap around a HIP API call. It then checks the return type and exits the program if there has been an error. For example we wrap a `hipmalloc` call with hipcheck as follows:\n",
    "\n",
    "```\n",
    "call hipcheck(hipmalloc(A_d, M, N))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cdb596-6fdc-4d8d-adf0-aedc19fe6a2e",
   "metadata": {},
   "source": [
    "## Code validation\n",
    "\n",
    "It is important to make sure that the output of the compuation is accurate for every element in the output. A wrong answer can be computed very quickly but it is of no use! In the file [math_utils.f90](math_utils.f90) is a function called `check_tensor_addition_2D` that iterates over every point in the output tensor $C(i,j)$ and checks to see each point is within an error margin of $A(i,j)+B(i,j)$. The function has the following signature, where **A**, **B**, and **C** are arrays on the host. It has the following signature:\n",
    "\n",
    "```Fortran\n",
    "function check_tensor_addition_2D(A, B, C, eps_mult) result(success)\n",
    "            !! Function to check the outcome of tensor addition\n",
    "            !! only check the host arrays\n",
    "\n",
    "            real(kind=c_float), dimension(:,:), intent(in), pointer :: A, B, C\n",
    "        \n",
    "            real, intent(in) :: eps_mult\n",
    "                !! Epsilon multiplier, how many floating point spacings\n",
    "                !! can the computed answer be from our benchmark answer\n",
    "```\n",
    "\n",
    "and we import it as the function `check` within the two programs\n",
    "\n",
    "```Fortran\n",
    "! Maths check\n",
    "    use math_utils, only : check => check_tensor_addition_2D\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44821c62-8e74-4b72-aba0-b15b85f4fb6c",
   "metadata": {},
   "source": [
    "## Fortran interface to kernel launch function\n",
    "\n",
    "Hipfort doesn't yet have a way to launch kernels, however passing pointers from Fortran to C/C++ functions is straightforward, and from C/C++ code we can launch kernels. In the file [kernel_code.cpp](kernel_code.cpp) is a function called `launch_kernel_hip` that does the job of launching kernels. It has the following signature:\n",
    "\n",
    "```Fortran\n",
    "    void launch_kernel_hip(\n",
    "            float_type* A, \n",
    "            float_type* B,\n",
    "            float_type* C,\n",
    "            int M,\n",
    "            int N) {\n",
    "```\n",
    "\n",
    "and we have type defined `float_type` as `float` earlier in the file.\n",
    "\n",
    "```C++\n",
    "typedef float float_type;\n",
    "```\n",
    "\n",
    "In order to call this function from Fortran we define an `interface` to the function within the programs of [tensoradd_hip_cptr.f90](tensoradd_hip_cptr.f90) and [tensoradd_hip_fptr.f90](tensoradd_hip_fptr.f90) as follows:\n",
    "\n",
    "```Fortran\n",
    "    interface\n",
    "        ! A C function with void return type\n",
    "        ! is regarded as a subroutine in Fortran \n",
    "        subroutine launch_kernel_hip(A, B, C, M, N) bind(C)\n",
    "            use iso_c_binding\n",
    "            ! Fortran passes arguments by reference as the default\n",
    "            ! Arguments must have the \"value\" option present to pass by value\n",
    "            ! Otherwise launch_kernel will receive pointers of type void**\n",
    "            ! instead of void*\n",
    "            type(c_ptr), intent(in), value :: A, B, C\n",
    "            integer(c_int), intent(in), value :: M, N\n",
    "        end subroutine\n",
    "        \n",
    "    end interface\n",
    "\n",
    "```\n",
    "\n",
    "Note the presence of the `value` option for the input arguments. This is so we pass arguments by `value` instead of by **reference** (the default). If we didn't have the value keyword the C function would receive a reference (or pointer to the variables) instead of a copy of the variables. In the case of `launch_kernel_hip` without the `value` keyword in the interface then A would be of type `void**` instead of `void*`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594eb06-9d38-4651-a518-ba28488f379f",
   "metadata": {},
   "source": [
    "## Select and manage a HIP device\n",
    "\n",
    "Each HIP compute device has a resource manager called a `primary context` that keeps track of all the resources allocated on that device. Host threads share access to the primary contexts in a way that is (or at least is intended to be!) thread safe. This means that each host thread in an application is **free to choose** which device to use. Usually the HIP runtime is initialised (primary contexts are created) and a host thread is **connected** to the first available device (device 0) whenever that host thread makes its first call to a HIP function. For environments where there are multiple devices it is **good practice** to explicity initialize the HIP API and be specific about which device you would like the host thread to connect to. In the file [hip_utils.f90](hip_utils.f90) are two subroutines `init_gpu` and `reset_gpu` that provide a way to choose a GPU and reset (release all resources) in the selected device's primary context. The first statement after variable declarations in  [tensoradd_hip_cptr.f90](tensoradd_hip_cptr.f90) and [tensoradd_hip_fptr.f90](tensoradd_hip_fptr.f90) is to initialize HIP and choose the GPU.\n",
    "\n",
    "```Fortran\n",
    "    ! Find and set the GPU device. Use device 0 by default\n",
    "    call init_gpu(0)   \n",
    "```\n",
    "\n",
    "The argument to init_gpu is the desired index of the device that we'd like to use. Device indices start at 0 and in this instance we select the first available gpu with id 0.\n",
    "\n",
    "Inside the function `init_gpu` we initialize the HIP API using a call to `hipinit`.\n",
    "\n",
    "```Fortran\n",
    "call hipcheck(hipinit(0))\n",
    "```\n",
    "\n",
    "A call to `hipinit` only needs to be done once, so we have a variable `acquired` within the module to make sure of this. \n",
    "\n",
    "Next, we call `hipgetdevicecount` to poll the number of valid devices. If the desired index (the input argument to `init_gpu`) falls within the range of valid device then we call `hipsetdevice` to set the HIP device according to the desired device index. Any subsequent HIP calls from a host thread will then use the selected GPU.\n",
    "\n",
    "```Fortran\n",
    " ! Get the number of compute devices\n",
    "        call hipcheck(hipgetdevicecount(ndevices))\n",
    "            \n",
    "        if ((dev_id .ge. 0) .and. (dev_id .lt. ndevices)) then\n",
    "            ! Choose a compute device\n",
    "            call hipcheck(hipsetdevice(dev_id))\n",
    "        else\n",
    "            write(error_unit,*) 'Error, dev_id was not inside the range of available devices.'\n",
    "            stop 1\n",
    "        end if\n",
    "```\n",
    "\n",
    "The function `reset_gpu` in [hip_utils.f90](hip_utils.f90) calls `hipdevicesynchronize` to make sure the selected GPU device is finished with all pending activity, then it calls `hipdevicereset` to release all resources in the primary context. \n",
    "\n",
    "```Fortran\n",
    "        ! Release all resources on the gpu\n",
    "        if (acquired) then\n",
    "            ! Make sure the GPU is finished\n",
    "            ! with all pending activity\n",
    "            call hipcheck(hipdevicesynchronize())\n",
    "\n",
    "            ! Now free all resources on the primary context\n",
    "            ! of the selected GPU\n",
    "            call hipcheck(hipdevicereset())\n",
    "        end if\n",
    "```\n",
    "\n",
    "It is **best practice** to reset the compute device at the end of the computation, but make sure that no other threads are using resources on that GPU when you do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f2d9f-7768-40ed-a613-0f7397e7d99d",
   "metadata": {},
   "source": [
    "## Memory on the device\n",
    "\n",
    "### Standard data types on the host\n",
    "\n",
    "Next, we allocate memory for the tensors on both the host and the compute device. Fortran has the ability to change, with a compiler flag, how many bytes are used `real` and `integer` types. HIP kernels need fixed data types, so when you allocate arrays that will be used to interact with device allocations, it is **best practice** in Fortran code to use array data types whose number of bytes **do not change**. We use the `c_float` kind from the `iso_c_binding` module to make sure the host arrays are of the data type that is synonymous with `float` in C code.\n",
    "\n",
    "```Fortran\n",
    "real(kind=c_float), dimension(:,:), pointer :: A_h, B_h, C_h\n",
    "\n",
    "! Allocate memory on host \n",
    "allocate(A_h(M,N), B_h(M, N), C_h(M,N))\n",
    "```\n",
    "\n",
    "### Variable naming convention\n",
    "\n",
    "Notice the `_h` suffix on variable names. In this module we choose to put a `_h` suffix on memory allocations that reside on the host and a `_d` suffix for memory allocations that reside on the compute device. It is a variable naming convention that makes it easier to see what memory is allocated where."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe3074-8276-4110-b1b9-76d98893e95b",
   "metadata": {},
   "source": [
    "### C pointers and Fortran pointers\n",
    "\n",
    "Both C pointers (`type(c_ptr)`) and Fortran pointers can be used as handles to memory allocations on the compute device. C pointers are flexible but not very safe. Fortran pointers are also not very safe but additionally  retain information on the shape, data type, and size of the allocation.\n",
    "\n",
    "In [tensoradd_hip_cptr.f90](tensoradd_hip_cptr.f90) we use C pointers for memory allocations to tensors **A**, **B**, and **C** on the compute device\n",
    "\n",
    "```Fortran\n",
    "    ! C Pointers to memory allocations on the device\n",
    "    type(c_ptr) :: A_d, B_d, C_d\n",
    "```\n",
    "\n",
    "and in [tensoradd_hip_fptr.f90](tensoradd_hip_fptr.f90) we use Fortran pointers.\n",
    "\n",
    "```Fortran\n",
    "    ! Fortran pointers to memory allocations on the device\n",
    "    real(kind=c_float), dimension(:,:), pointer :: A_d, B_d, C_d\n",
    "```\n",
    "\n",
    "### Allocate device memory\n",
    "\n",
    "The `hipmalloc` function allocates memory in the **global** memory space on the compute device. This memory is the largest (and slowest) memory on the compute device. Memory allocated with `hipmalloc` is accessible from every kernel that runs on the compute device but not from the host. \n",
    "\n",
    "When using hipmalloc with **C pointers** we need to specify how many bytes to reserve. The `sizeof` function returns the number of bytes allocated for a Fortran pointer. In [tensoradd_hip_cptr.f90](tensoradd_hip_cptr.f90) we use the bytes allocated for the host arrays as an input argument when allocating **A_d**, **B_d**, and **C_d**.\n",
    "\n",
    "```Fortran\n",
    "    ! Allocate tensors on the GPU\n",
    "    call hipcheck(hipmalloc(A_d, sizeof(A_h)))\n",
    "    call hipcheck(hipmalloc(B_d, sizeof(B_h)))\n",
    "    call hipcheck(hipmalloc(C_d, sizeof(C_h)))\n",
    "```\n",
    "\n",
    "Fortran pointers need **elements** (not bytes) as the input argument for allocation with `hipmalloc`. In [tensoradd_hip_fptr.f90](tensoradd_hip_fptr.f90) we specify the size of the arrays to allocate in elements along each dimension.\n",
    "\n",
    "```Fortran\n",
    "    ! Allocate memory on the GPU\n",
    "    call hipcheck(hipmalloc(A_d, M, N))\n",
    "    call hipcheck(hipmalloc(B_d, M, N))\n",
    "    call hipcheck(hipmalloc(C_d, M, N))\n",
    "```\n",
    "\n",
    "There are additional ways to allocate memory with Fortran pointers. For example we could have used the `hipmalloc_r4_c_size_t` function to allocate the 2D arrays, each element using 4 bytes, and having integer variables of kind `c_size_t` to specify dimensions.\n",
    "\n",
    "\n",
    "```Fortran\n",
    "    ! Could have also done this for the allocate instead\n",
    "    call hipcheck(hipmalloc_r4_2_c_size_t(A_d, int(M_in, c_size_t), int(N_in, c_size_t)))\n",
    "    call hipcheck(hipmalloc_r4_2_c_size_t(B_d, int(M_in, c_size_t), int(N_in, c_size_t)))\n",
    "    call hipcheck(hipmalloc_r4_2_c_size_t(C_d, int(M_in, c_size_t), int(N_in, c_size_t)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162382e-f17a-47a3-b5e3-c626784b34ec",
   "metadata": {},
   "source": [
    "### De-allocate device memory\n",
    "\n",
    "When device memory is no longer needed, the **hipfree** function deallocates device memory with both C and Fortran pointers.\n",
    "\n",
    "```Fortran\n",
    "    ! Free allocations on the GPU\n",
    "    call hipcheck(hipfree(A_d))\n",
    "    call hipcheck(hipfree(B_d))\n",
    "    call hipcheck(hipfree(C_d))\n",
    "```\n",
    "\n",
    "It is **best practice** to make sure pointers are set to null when they no longer point to something. For Fortran pointers ([tensoradd_hip_fptr.f90](tensoradd_hip_fptr.f90)) we use the `nullify` function\n",
    "\n",
    "```Fortran\n",
    "    ! It is best practice to nullify all pointers \n",
    "    ! once we are done with them \n",
    "    nullify(A_h, B_h, C_h, A_d, B_d, C_d)\n",
    "```\n",
    "\n",
    "and for C pointers ([tensoradd_hip_cptr.f90](tensoradd_hip_cptr.f90)) we set them to `c_null_ptr`.\n",
    "\n",
    "```Fortran\n",
    "    ! Set C pointers to null as well\n",
    "    A_d = c_null_ptr\n",
    "    B_d = c_null_ptr\n",
    "    C_d = c_null_ptr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882295bc-4ff5-4368-8788-caa3f859cabb",
   "metadata": {},
   "source": [
    "## Memory copies between host and device\n",
    "\n",
    "Memory can be copied between host and device allocations, or between device allocations. After filling arrays **A_h** and **B_h** we proceed to copy them to the device allocations **A_d** and **B_d**.\n",
    "\n",
    "### Copy from host to device\n",
    "\n",
    "The `hipmemcpy` function can use either C pointers or Fortran pointers. Here is the code to copy from host to device using C pointers.\n",
    "\n",
    "```Fortran\n",
    "    ! Copy memory from the host to the device \n",
    "    call hipcheck(hipmemcpy(A_d, c_loc(A_h), sizeof(A_h), hipmemcpyhosttodevice))\n",
    "    call hipcheck(hipmemcpy(B_d, c_loc(B_h), sizeof(B_h), hipmemcpyhosttodevice))\n",
    "```\n",
    "\n",
    "Each `hipmemcpy` call has a additional flag to specify the direction of the copy. There are five options available:\n",
    "\n",
    "* `hipmemcpyhosttohost`\n",
    "* `hipmemcpyhosttodevice`\n",
    "* `hipmemcpydevicetohost`\n",
    "* `hipmemcpydevicetodevice`\n",
    "* `hipmemcpydefault`\n",
    "\n",
    "The `hipmemcpydefault` option tries to infer the direction of transfer from the memory spaces of the input pointers. It is less readable however.\n",
    "\n",
    "Hipmemcpy also works with Fortran pointers, though when specifying the size to copy we specify **elements** instead of **bytes**! Notice the use of `size` instead of `sizeof` to specify elements instead of bytes.\n",
    "\n",
    "```Fortran\n",
    "    call hipcheck(hipmemcpy(A_d, A_h, size(A_h), hipmemcpyhosttodevice))\n",
    "    call hipcheck(hipmemcpy(B_d, B_h, size(B_h), hipmemcpyhosttodevice))\n",
    "```\n",
    "\n",
    "In the case of Fortran pointers we could have also used `hipmemcpy` functions that are specific to the arrays in question, for example we could also have done this.\n",
    "\n",
    "```Fortran\n",
    "    ! Could also have done this for the copy instead\n",
    "    !call hipcheck(hipmemcpy_r4_2_c_size_t(A_d, A_h, &\n",
    "    !    int(size(A_h), c_size_t), hipmemcpyhosttodevice))\n",
    "    !call hipcheck(hipmemcpy_r4_2_c_size_t(B_d, B_h, &\n",
    "    !    int(size(B_h), c_size_t), hipmemcpyhosttodevice))\n",
    "```\n",
    "\n",
    "### Copy from device to host\n",
    "\n",
    "After running the kernel, we copy **C_d** back to **C_h**, using either C pointers,\n",
    "\n",
    "```Fortran\n",
    "    ! Copy memory from the device to the host\n",
    "    call hipcheck(hipmemcpy(c_loc(C_h), C_d, sizeof(C_h), hipmemcpydevicetohost))\n",
    "```\n",
    "or Fortran pointers\n",
    "\n",
    "```Fortran\n",
    "    ! Copy from the device result back to the host\n",
    "    call hipcheck(hipmemcpy(C_h, C_d, size(C_d), hipmemcpydevicetohost))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914889ca-e7fe-4cbb-ac5f-adb6353039c4",
   "metadata": {},
   "source": [
    "## Kernel source and launch\n",
    "\n",
    "### Call the kernel launch function\n",
    "\n",
    "Since the Hipfort API doesn't have the functionality to define and launch kernels, we use the C function `launch_kernel_hip` to launch the kernel. This function has as the input argument C pointers for **A**, **B**, and **C** on the device and the integers **M** and **N** for the array sizes. In [tensoradd_hip_cptr.f90](tensoradd_hip_cptr.f90) we can just use the pointers **A_d**, **B_d**, and **C_d** directly while taking special care to convert the integer arguments to the type required by the function.\n",
    "\n",
    "```Fortran\n",
    "    ! Call the C function that launches the kernel\n",
    "    call launch_kernel_hip( &\n",
    "        A_d, &\n",
    "        B_d, &\n",
    "        C_d, &\n",
    "        int(M, c_int), &\n",
    "        int(N, c_int) &\n",
    "    )\n",
    "```\n",
    "\n",
    "In [tensoradd_hip_fptr.f90](tensoradd_hip_fptr.f90) we must use `c_loc` to get the C pointer that underlies the Fortran pointers.\n",
    "\n",
    "```Fortran\n",
    "    ! Call the C function that launches the kernel\n",
    "    call launch_kernel_hip( &\n",
    "        c_loc(A_d), &\n",
    "        c_loc(B_d), &\n",
    "        c_loc(C_d), &\n",
    "        int(M, c_int), &\n",
    "        int(N, c_int) &\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5c33e-2c80-42e2-b194-9ed8b55abd58",
   "metadata": {},
   "source": [
    "### Kernel launch function\n",
    "\n",
    "Let's examine the file [kernel_code.cpp](kernel_code.cpp). \n",
    "\n",
    "#### C linkage\n",
    "\n",
    "The kernel launch function `launch_kernel_hip` is wrapped in an `extern \"C\"` code block to ensure the function is compiled with C linkage, meaning it's name doesn't get mangled and is accessible from Fortran.\n",
    "\n",
    "```C++\n",
    "// C function to call the tensoradd_2D kernel\n",
    "extern \"C\" {\n",
    "\n",
    "    void launch_kernel_hip(\n",
    "            float_type* A, \n",
    "            float_type* B,\n",
    "            float_type* C,\n",
    "            int M,\n",
    "            int N) {\n",
    "```\n",
    "\n",
    "#### Role of the kernel launch function\n",
    "\n",
    "From the **GPU Computing Fundamentals** section we have the following diagram of a Grid that is made up of blocks.\n",
    "\n",
    "<figure style=\"margin: 1em; margin-left:auto; margin-right:auto; width:90%;\">\n",
    "    <img src=\"../images/Grid.svg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">A Grid in the context of GPU computing. Grids are made up of Blocks and Blocks are made up of Threads</figcaption>\n",
    "</figure>\n",
    "\n",
    "It is the job of the kernel launch function to: \n",
    "\n",
    "* Pass arguments to the kernel \n",
    "* Determine the block size, (number of threads along each dimension of the block)\n",
    "* Determine the grid size, (number of blocks along each dimension of the grid)\n",
    "* Launch the kernel and examine launch errors\n",
    "* Optionally synchronize the device\n",
    "\n",
    "#### Define block size and grid size\n",
    "\n",
    "The `dim3` structure (with fields `x`, `y`, `z`) is used to specify the block size and the number of blocks per dimension.\n",
    "\n",
    "```C++       \n",
    "        // Grid size\n",
    "        dim3 global_size = {\n",
    "            (uint32_t)(M), \n",
    "            (uint32_t)(N)\n",
    "        }; \n",
    "        \n",
    "        // Block size, \n",
    "        dim3 block_size = {8,8,1};\n",
    "        \n",
    "        // Number of blocks in each dimension\n",
    "        dim3 nblocks = {\n",
    "            global_size.x/block_size.x,\n",
    "            global_size.y/block_size.y,\n",
    "            1\n",
    "        };\n",
    "```\n",
    "\n",
    "We must always have an integer number of blocks along the grid. Sometimes this means making a grid that is larger than we need. This is ok provided we build protection into the kernel so we don't run off the end of the arrays. \n",
    "    \n",
    "```C++\n",
    "        // Make sure there are enough blocks\n",
    "        if (global_size.x % block_size.x) nblocks.x += 1;\n",
    "        if (global_size.y % block_size.y) nblocks.y += 1;\n",
    "        if (global_size.z % block_size.z) nblocks.z += 1;\n",
    "```\n",
    "\n",
    "#### Shared memory\n",
    "\n",
    "HIP provides the ability to define a small amount of **shared memory** that is available to all threads in a block. This memory is fast and can be used as a small scratch space. We don't need shared memory for this example so we specify `0` as the number of bytes to allocate for shared memory.\n",
    "\n",
    "\n",
    "```C++\n",
    "        // Decide on the number of bytes to allocate for shared memory\n",
    "        size_t sharedMemBytes = 0;\n",
    "```\n",
    "\n",
    "#### Kernel launch with hipLaunchKernelGGL\n",
    "\n",
    "Finally we get to launch the kernel itself. There are a few ways to do this, here we use the **hipLaunchKernelGGL** macro to launch the kernel `tensoradd_2D` with the specified block and grid size along with kernel arguments. A `stream` in HIP can be thought of as a work queue to which we submit work, we use stream 0 which is the default or null stream.\n",
    "\n",
    "```C++\n",
    "        // Launch the kernel\n",
    "        hipLaunchKernelGGL(\n",
    "                // Kernel name\n",
    "                tensoradd_2D,\n",
    "                // Number of blocks per dimension\n",
    "                nblocks,\n",
    "                // Number of threads along each dimension of the block\n",
    "                block_size,\n",
    "                // Number of bytes dynamically allocated for shared memory\n",
    "                sharedMemBytes,\n",
    "                // Stream to use (0 is the default or null stream)\n",
    "                0,\n",
    "                // Kernel arguments\n",
    "                A, B, C,\n",
    "                M, N);\n",
    "```\n",
    "\n",
    "#### Kernel launch with CUDA syntax\n",
    "\n",
    "One can also use the CUDA-like triple-chevron syntax to launch a kernel. This is not ANSI C++ compliant though, which isn't too much of a problem because only compilers (hipcc, nvcc) that understand triple chevrons will be used to compile this source file anyway.\n",
    "\n",
    "```C++\n",
    "        // The triple-chevron (non C++ compliant) way of launching kernels\n",
    "        tensoradd_2d<<<nblocks, block_size, sharedMemBytes, 0>>>(A, B, C, M, N);\n",
    "```\n",
    "\n",
    "#### Check kernel launch\n",
    "\n",
    "We use the `hipGetLastError` function to see if there were any problems arising from kernel launch. The macro `HIPCHECK` is defined earlier in the file [kernel_code.cpp](kernel_code.cpp) and behaves similarly to the Fortran function `hipcheck` from `hipfort_check`.\n",
    "\n",
    "```C++\n",
    "        // Make sure the kernel launch went ok\n",
    "        HIPCHECK(hipGetLastError());\n",
    "```\n",
    "\n",
    "#### Synchronize the compute device\n",
    "\n",
    "Finally, we use the `hipDeviceSynchronize` function to make sure that the kernel is finished before continuing. This step is not strictly necessary because the subsequent copy of **C_d** to **C_h** will use the same (null) stream and will block until the kernel is finished. \n",
    "\n",
    "```C++\n",
    "    \t// Wait for the kernel to finish\n",
    "    \tHIPCHECK(hipDeviceSynchronize()); \n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b5a3fd-8a35-41b0-8ea3-fc6cc00edaf5",
   "metadata": {},
   "source": [
    "## Code validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3608616-95bf-453f-8bd4-f35ed1bc39f5",
   "metadata": {},
   "source": [
    "## Resource cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3615d12d-72cc-472e-8660-49654962c510",
   "metadata": {},
   "source": [
    "## Use object oriented types for memory safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf05b02d-96f6-40f1-9840-1e586c1b6e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
