{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f7d7ba-cf7a-4b71-9148-9da66c58ac0c",
   "metadata": {},
   "source": [
    "# Introducing HIP\n",
    "\n",
    "HIP stands for the Heterogeneous Interface for Portability. HIP is part of ROCM, AMD's competitor to CUDA, and aims to make GPU's accessible through providing  a subset of capability formed from the fusion of the driver and runtime API's in CUDA. HIP calls have their own prefix (i.e **hipMalloc** instead of **cudaMalloc**) and they can serve as a **very thin** layer over a vendor's GPU library calls when using their backend. As such, this design philosophy currently allows HIP programs to use either an AMD, NVIDIA, or even an Intel accelerator as the compute device, while allowing the use of vendor-specific debugging and performance tools. HIP has a number of benefits that include:\n",
    "\n",
    "* A single source for programs and kernels.\n",
    "* The ability to use an Intel, NVIDIA, or AMD compute devices at full performance.\n",
    "* Easy-to-use API that is familiar in many ways to CUDA, with the ability to benefit from knowledge in established literature on GPU computing with CUDA.\n",
    "* Tools available to port code from CUDA to HIP.\n",
    "\n",
    "There are also some challenges to be considered when considering using HIP for your project.\n",
    "\n",
    "* The number of officially-supported devices and operating systems is quite low. See this [page](https://rocm.docs.amd.com/en/latest/release/gpu_os_support.html) for devices that are officially supported. Other recent AMD devices **do** generally work with ROCm, but with varying levels of functionality, and no official support. \n",
    "* Unlike OpenCL, only one type of compute device (i.e limited to a single vendor) is accessible to a HIP program at runtime. In order to change vendors or compute devices the program must be recompiled with a different backend.\n",
    "* The HIP API is still on the path to maturity. As such it is anticipated there will be bugs and things that might not work properly.\n",
    "* Not every CUDA API call is supported in HIP.\n",
    "* Not every HIP API call is supported in CUDA.\n",
    "\n",
    "## Portability considerations\n",
    "\n",
    "Performance with HIP or CUDA code often comes at the price of additional and sometimes vendor-specific complexity. One can use macros to ringfence NVIDIA-specific or AMD-specific code, however this can make the codebase messy. Keeping the codebase fairly simple and avoiding using advanced features is a good idea for portability and readability. An alternative is to abstract access to the GPU hardware within the program, and use separate code trees for specific vendors.\n",
    "\n",
    "## Hipfort as the Fortran interface to HIP\n",
    "\n",
    "Hipfort began its life as [hip-fortran](https://github.com/FluidNumerics/hip-fortran) that Joseph Schoonover constructed as a way to integrate Fortran applications with HIP. This codebase was adapted by AMD and became the [Hipfort](https://github.com/ROCm/hipfort) package that we are using in this course.\n",
    "\n",
    "Hipfort is the Fortran interface to HIP, and provides a way to access the compute power of a GPU and HIP and ROCm compute libraries from a Fortran program. It is designed to provide access to HIP and ROCm library calls. Supported libraries include:\n",
    "\n",
    "* HIP\n",
    "* hipBLAS and rocBLAS (Basic Linear Algebra)\n",
    "* hipFFT and rocFFT (Fast Fourier Transforms)\n",
    "* hipRAND and rocRAND (Random number generation)\n",
    "* hipSOLVER and rocSOLVER (Linear algebra solvers, AMD backends only at this point)\n",
    "* hipSPARSE and rocSPARSE (Implementation and tools to work with spare matrices.)\n",
    "\n",
    "The hip libraries can use multiple backends, while the rocm libraries are specific to AMD.\n",
    "\n",
    "### Technical capabilities\n",
    "\n",
    "Hipfort provides a way to work with devices, and create memory allocations. The machinery to launch kernels is left to C++ source code, however with the assistance of the [ISO_C_BINDING](https://gcc.gnu.org/onlinedocs/gfortran/ISO_005fC_005fBINDING.html) module, pointers to memory allocations on the GPU may be stored in a Fortran program. Using Fortran interfaces it is straightforward to call C/C++ functions from Fortran and launch kernels from within these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee5c63-2905-4a25-a551-93a573b1310c",
   "metadata": {},
   "source": [
    "### When to consider Hipfort\n",
    "\n",
    "The benefits of using Hipfort are:\n",
    "\n",
    "* The availability to call many (but not all) HIP functions from Fortran and reduce the number of times that C/C++ functions must be called.\n",
    "\n",
    "In addition to the challenges in using HIP, some additional challenges to be considered are:\n",
    "\n",
    "* Not many Fortran compilers are known to be supported by Hipfort at present. They are:\n",
    "    * GFortran\n",
    "    * Flang\n",
    " \n",
    "This means that in order to use Hipfort in your applications you need to be comfortable with using a limited number of compilers. Furthermore you also need to be comfortable with calling C/C++ functions from Fortran."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044ad661-5b02-4ed6-afba-4fee995030f1",
   "metadata": {},
   "source": [
    "<address>\n",
    "Written by Dr. Toby Potter of <a href=\"https://www.pelagos-consulting.com\">Pelagos Consulting and Education</a> and Dr. Joe Schoonover from <a href=\"https://www.fluidnumerics.com\">Fluid Numerics</a>. All trademarks mentioned in this page are the property of their prospective owners.\n",
    "</address> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
